

Correct
1 / 1 points
1. 
Why is a Deep Belief Network not a Boltzmann Machine ?

A DBN does not have hidden units.

A DBN is not a probabilistic model of the data.

All edges in a DBN are directed.

Some edges in a DBN are directed.
Correct 
In a Boltzmann Machine, all edges must be undirected. A DBN has directed edges from the top-level RBM to each subsequent layer below.

Incorrect
0 / 1 points
2. 
Brian looked at the direction of arrows in a DBN and was surprised to find that the data is at the "output". "Where is the input ?!", he exclaimed, "How will I give input to this model and get all those cool features?" In this context, which of the following statements are true? Check all that apply.

In order to get features h given some data v, he must perform inference to find out P(h|v). There is an easy exact way of doing this, just traverse the arrows in the opposite direction.
Un-selected is correct 

A DBN is a generative model of the data, which means that, its arrows define a way of generating data from a probability distribution, so there is no "input".
This should be selected 

In order to get features h given some data v, he must perform inference to find out P(h|v). There is an easy approximate way of doing this, just traverse the arrows in the opposite direction.
Correct 
Traversing arrows in the opposite direction is an approximate inference procedure.

A DBN is a generative model of the data and cannot be used to generate features for any given input. It can only be used to get features for data that was generated by the model.
This should not be selected 
This is not true. A DBN can be used to generate features for any given data vector by doing inference.

Correct
1 / 1 points
3. 
Suppose you wanted to learn a neural net classifier. You have data and labels. All you care about is predicting the labels accurately for a test set. How can pretraining help in getting better accuracy, even though it does not use any information about the labels ?

Pretraining will learn exactly the same features that a simple neural net would learn because after all, they are training on the same data set. But pretraining does not use the labels and hence it can prevent overfitting.

There is an assumption that pretraining will learn features that will be useful for discrimination and it would be difficult to learn these features using just the labels.
Correct 
This is correct. The whole idea behind pretraining is an assumption that features that are useful in a generative model will also be somewhat useful for discrimination. This is almost always true for real-world problems.

The objective function used during pretraining is the same as the one used during fine-tuning. So pretraining provides more updates towards solving the same optimization problem.

Pretraining will always learn features that will be useful for discrimination, no matter what the discriminative task is.

Correct
1 / 1 points
4. 
Why does pretraining help more when the network is deep ?

Deeper nets have more parameters than shallow ones and they overfit easily. Therefore, initializing them sensibly is important.
Correct 
More parameters means that the model can find ingenious ways of overfitting by learning features that don't generalize well. Pretraining can initialize the weights in a proper region of weight space so that the features learned are not too bad.

As nets get deeper, contrastive divergence objective used during pretraining gets closer to the classification objective.
Un-selected is correct 

Backpropagation algorithm cannot give accurate gradients for very deep networks. So it is important to have good initializtions, especially, for the lower layers.
Un-selected is correct 

During backpropagation in very deep nets, the lower level layers get very small gradients, making it hard to learn good low-level features. Since pretraining starts those low-level features off at a good point, there is a big win.
Correct 
Lower level layers can get very small gradients, especially if saturating hidden units are used (such as logistic or tanh units). Pretraining can initialize the weights in a proper region of weight space so that the features don't have to start learning from scratch.

Incorrect
0 / 1 points
5. 
The energy function for binary RBMs goes by

E(v,h)=−∑ivibi−∑jhjaj−∑i,jviWijhj
When modeling real-valued data (i.e., when v is a real-valued vector not a binary one) we change it to

E(v,h)=∑i(vi−bi)22σ2i−∑jhjaj−∑i,jviσiWijhj
Why can't we still use the same old one ?

If we continue to use the same one, then in general, there will be infinitely many v's and h's such that, E(v,h) will be infinitely small (close to −∞). The probability distribution resulting from such an energy function is not useful for modeling real data.
This should be selected 

If the model assigns an energy e1 to state v1,h, and e2 to state v2,h, then it would assign energy (e1+e2)/2 to state (v1+v2)/2,h. This does not make sense for the kind of distributions we usually want to model.
This should be selected 

If we use the old one, the real-valued vectors would end up being constrained to be binary.
Un-selected is correct 

Probability distributions over real-valued data can only be modeled by having a conditional Gaussian distribution over them. So we have to use a quadratic term.
This should not be selected 
This is not true. Distributions over real-valued data can be modelled by a multitude of distributions or combinations thereof. The choice of Gaussian distribution is arbitrary.
